{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three types of window functions:\n",
    "1) Analytical - lead(),lag(),cume_dist()\n",
    "2) Aggregate - average, sum, min, max\n",
    "3) Ranking - row_number(), rank(),dense_rank()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.driver.host\", \"localhost\").appName(\"SparkByExamples.com\").getOrCreate()\n",
    "conf = pyspark.SparkConf()\n",
    "spark_context = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData = ((\"Nitya\", 28, \"Sales\", 3000),\n",
    "(\"Abhishek\", 33, \"Sales\", 4600),\n",
    "(\"Sandeep\", 40, \"Sales\", 4100),\n",
    "(\"Rakesh\", 25, \"Finance\", 3000),\n",
    "(\"Ram\", 28, \"Sales\", 3000),\n",
    "(\"Srishti\", 46, \"Management\", 3300),\n",
    "(\"Arbind\", 26, \"Finance\", 3900),\n",
    "(\"Hitesh\", 30, \"Marketing\", 3000),\n",
    "(\"Kailash\", 29, \"Marketing\", 2000),\n",
    "(\"Sushma\", 39, \"Sales\", 4100)\n",
    ")\n",
    "columns = [\"Employee_name\",\"Age\",\"Department\",\"Salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=sampleData, schema=columns)\n",
    "windowpartition = Window.partitionBy(\"Department\").orderBy(\"Age\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+\n",
      "|Employee_name|Age|Department|Salary|\n",
      "+-------------+---+----------+------+\n",
      "|Nitya        |28 |Sales     |3000  |\n",
      "|Abhishek     |33 |Sales     |4600  |\n",
      "|Sandeep      |40 |Sales     |4100  |\n",
      "|Rakesh       |25 |Finance   |3000  |\n",
      "|Ram          |28 |Sales     |3000  |\n",
      "|Srishti      |46 |Management|3300  |\n",
      "|Arbind       |26 |Finance   |3900  |\n",
      "|Hitesh       |30 |Marketing |3000  |\n",
      "|Kailash      |29 |Marketing |2000  |\n",
      "|Sushma       |39 |Sales     |4100  |\n",
      "+-------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Employee_name: string, Age: bigint, Department: string, Salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cume_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+---------+\n",
      "|Employee_name|Age|Department|Salary|cume_dist|\n",
      "+-------------+---+----------+------+---------+\n",
      "|       Rakesh| 25|   Finance|  3000|      0.5|\n",
      "|       Arbind| 26|   Finance|  3900|      1.0|\n",
      "|      Srishti| 46|Management|  3300|      1.0|\n",
      "|      Kailash| 29| Marketing|  2000|      0.5|\n",
      "|       Hitesh| 30| Marketing|  3000|      1.0|\n",
      "|        Nitya| 28|     Sales|  3000|      0.4|\n",
      "|          Ram| 28|     Sales|  3000|      0.4|\n",
      "|     Abhishek| 33|     Sales|  4600|      0.6|\n",
      "|       Sushma| 39|     Sales|  4100|      0.8|\n",
      "|      Sandeep| 40|     Sales|  4100|      1.0|\n",
      "+-------------+---+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowpartition)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+\n",
      "|Employee_name|Age|Department|Salary| Lag|\n",
      "+-------------+---+----------+------+----+\n",
      "|       Rakesh| 25|   Finance|  3000|null|\n",
      "|       Arbind| 26|   Finance|  3900|null|\n",
      "|      Srishti| 46|Management|  3300|null|\n",
      "|      Kailash| 29| Marketing|  2000|null|\n",
      "|       Hitesh| 30| Marketing|  3000|null|\n",
      "|        Nitya| 28|     Sales|  3000|null|\n",
      "|          Ram| 28|     Sales|  3000|null|\n",
      "|     Abhishek| 33|     Sales|  4600|3000|\n",
      "|       Sushma| 39|     Sales|  4100|3000|\n",
      "|      Sandeep| 40|     Sales|  4100|4600|\n",
      "+-------------+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  lag\n",
    "df.withColumn(\"Lag\",lag(\"Salary\",2).over(windowpartition)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+----+\n",
      "|Employee_name|Age|Department|Salary|Lead|\n",
      "+-------------+---+----------+------+----+\n",
      "|       Rakesh| 25|   Finance|  3000|null|\n",
      "|       Arbind| 26|   Finance|  3900|null|\n",
      "|      Srishti| 46|Management|  3300|null|\n",
      "|      Kailash| 29| Marketing|  2000|null|\n",
      "|       Hitesh| 30| Marketing|  3000|null|\n",
      "|        Nitya| 28|     Sales|  3000|4600|\n",
      "|          Ram| 28|     Sales|  3000|4100|\n",
      "|     Abhishek| 33|     Sales|  4600|4100|\n",
      "|       Sushma| 39|     Sales|  4100|null|\n",
      "|      Sandeep| 40|     Sales|  4100|null|\n",
      "+-------------+---+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "df.withColumn(\"Lead\",lead(\"salary\",2).over(windowpartition)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANKING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData = ((101, \"Ram\", \"Biology\", 80),\n",
    "(103, \"Sita\", \"Social Science\", 78),\n",
    "(104, \"Lakshman\", \"Sanskrit\", 58),\n",
    "(102, \"Kunal\", \"Phisycs\", 89),\n",
    "(101, \"Ram\", \"Biology\", 80),\n",
    "(106, \"Srishti\", \"Maths\", 70),\n",
    "(108, \"Sandeep\", \"Physics\", 75),\n",
    "(107, \"Hitesh\", \"Maths\", 88),\n",
    "(109, \"Kailash\", \"Maths\", 90),\n",
    "(105, \"Abhishek\", \"Social Science\", 84)\n",
    ")\n",
    "columns = [\"Roll_No\", \"Student_Name\", \"Subject\", \"Marks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Roll_No: long (nullable = true)\n",
      " |-- Student_Name: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Marks: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame(data = sampleData, schema=columns)\n",
    "windowpartition = Window.partitionBy(\"Subject\").orderBy(\"Marks\")\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|\n",
      "+-------+------------+--------------+-----+\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    103|        Sita|Social Science|   78|\n",
      "|    104|    Lakshman|      Sanskrit|   58|\n",
      "|    102|       Kunal|       Phisycs|   89|\n",
      "|    101|         Ram|       Biology|   80|\n",
      "|    106|     Srishti|         Maths|   70|\n",
      "|    108|     Sandeep|       Physics|   75|\n",
      "|    107|      Hitesh|         Maths|   88|\n",
      "|    109|     Kailash|         Maths|   90|\n",
      "|    105|    Abhishek|Social Science|   84|\n",
      "+-------+------------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rownumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|row_number|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "|    101|         Ram|       Biology|   80|         1|\n",
      "|    101|         Ram|       Biology|   80|         2|\n",
      "|    106|     Srishti|         Maths|   70|         1|\n",
      "|    107|      Hitesh|         Maths|   88|         2|\n",
      "|    109|     Kailash|         Maths|   90|         3|\n",
      "|    102|       Kunal|       Phisycs|   89|         1|\n",
      "|    108|     Sandeep|       Physics|   75|         1|\n",
      "|    104|    Lakshman|      Sanskrit|   58|         1|\n",
      "|    103|        Sita|Social Science|   78|         1|\n",
      "|    105|    Abhishek|Social Science|   84|         2|\n",
      "+-------+------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "df2.withColumn(\"row_number\",row_number().over(windowpartition)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+----+\n",
      "|Roll_No|Student_Name|       Subject|Marks|rank|\n",
      "+-------+------------+--------------+-----+----+\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    101|         Ram|       Biology|   80|   1|\n",
      "|    106|     Srishti|         Maths|   70|   1|\n",
      "|    107|      Hitesh|         Maths|   88|   2|\n",
      "|    109|     Kailash|         Maths|   90|   3|\n",
      "|    102|       Kunal|       Phisycs|   89|   1|\n",
      "|    108|     Sandeep|       Physics|   75|   1|\n",
      "|    104|    Lakshman|      Sanskrit|   58|   1|\n",
      "|    103|        Sita|Social Science|   78|   1|\n",
      "|    105|    Abhishek|Social Science|   84|   2|\n",
      "+-------+------------+--------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  rank\n",
    "df2.withColumn(\"rank\",rank().over(windowpartition)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+--------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|per_rank|\n",
      "+-------+------------+--------------+-----+--------+\n",
      "|    101|         Ram|       Biology|   80|     0.0|\n",
      "|    101|         Ram|       Biology|   80|     0.0|\n",
      "|    106|     Srishti|         Maths|   70|     0.0|\n",
      "|    107|      Hitesh|         Maths|   88|     0.5|\n",
      "|    109|     Kailash|         Maths|   90|     1.0|\n",
      "|    102|       Kunal|       Phisycs|   89|     0.0|\n",
      "|    108|     Sandeep|       Physics|   75|     0.0|\n",
      "|    104|    Lakshman|      Sanskrit|   58|     0.0|\n",
      "|    103|        Sita|Social Science|   78|     0.0|\n",
      "|    105|    Abhishek|Social Science|   84|     1.0|\n",
      "+-------+------------+--------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  percent_rank\n",
    "df2.withColumn(\"per_rank\",percent_rank().over(windowpartition)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+-----+--------+\n",
      "|Roll_No|Student_Name|       Subject|Marks|per_rank|\n",
      "+-------+------------+--------------+-----+--------+\n",
      "|    101|         Ram|       Biology|   80|       1|\n",
      "|    101|         Ram|       Biology|   80|       1|\n",
      "|    106|     Srishti|         Maths|   70|       1|\n",
      "|    107|      Hitesh|         Maths|   88|       2|\n",
      "|    109|     Kailash|         Maths|   90|       3|\n",
      "|    102|       Kunal|       Phisycs|   89|       1|\n",
      "|    108|     Sandeep|       Physics|   75|       1|\n",
      "|    104|    Lakshman|      Sanskrit|   58|       1|\n",
      "|    103|        Sita|Social Science|   78|       1|\n",
      "|    105|    Abhishek|Social Science|   84|       2|\n",
      "+-------+------------+--------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  dense_rank\n",
    "df2.withColumn(\"per_rank\",dense_rank().over(windowpartition)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData = ((\"Ram\", \"Sales\", 3000),\n",
    "(\"Meena\", \"Sales\", 4600),\n",
    "(\"Abhishek\", \"Sales\", 4100),\n",
    "(\"Kunal\", \"Finance\", 3000),\n",
    "(\"Ram\", \"Sales\", 3000),\n",
    "(\"Srishti\", \"Management\", 3300),\n",
    "(\"Sandeep\", \"Finance\", 3900),\n",
    "(\"Hitesh\", \"Marketing\", 3000),\n",
    "(\"Kailash\", \"Marketing\", 2000),\n",
    "(\"Shyam\", \"Sales\", 4100)\n",
    ")\n",
    "columns = [\"Employee_Name\", \"Department\", \"Salary\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame(data=sampleData,schema=columns)\n",
    "df3.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|Employee_Name|Department|Salary|\n",
      "+-------------+----------+------+\n",
      "|          Ram|     Sales|  3000|\n",
      "|        Meena|     Sales|  4600|\n",
      "|     Abhishek|     Sales|  4100|\n",
      "|        Kunal|   Finance|  3000|\n",
      "|          Ram|     Sales|  3000|\n",
      "|      Srishti|Management|  3300|\n",
      "|      Sandeep|   Finance|  3900|\n",
      "|       Hitesh| Marketing|  3000|\n",
      "|      Kailash| Marketing|  2000|\n",
      "|        Shyam|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowPartitionAgg = Window.partitionBy(\"Department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+\n",
      "|Employee_Name|Department|Salary|   avg|\n",
      "+-------------+----------+------+------+\n",
      "|        Kunal|   Finance|  3000|3450.0|\n",
      "|      Sandeep|   Finance|  3900|3450.0|\n",
      "|      Srishti|Management|  3300|3300.0|\n",
      "|       Hitesh| Marketing|  3000|2500.0|\n",
      "|      Kailash| Marketing|  2000|2500.0|\n",
      "|          Ram|     Sales|  3000|3760.0|\n",
      "|        Meena|     Sales|  4600|3760.0|\n",
      "|     Abhishek|     Sales|  4100|3760.0|\n",
      "|          Ram|     Sales|  3000|3760.0|\n",
      "|        Shyam|     Sales|  4100|3760.0|\n",
      "+-------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.withColumn(\"avg\",avg(col(\"salary\")).over(windowPartitionAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|Employee_Name|Department|Salary|  Sum|\n",
      "+-------------+----------+------+-----+\n",
      "|        Kunal|   Finance|  3000| 6900|\n",
      "|      Sandeep|   Finance|  3900| 6900|\n",
      "|      Srishti|Management|  3300| 3300|\n",
      "|       Hitesh| Marketing|  3000| 5000|\n",
      "|      Kailash| Marketing|  2000| 5000|\n",
      "|          Ram|     Sales|  3000|18800|\n",
      "|        Meena|     Sales|  4600|18800|\n",
      "|     Abhishek|     Sales|  4100|18800|\n",
      "|          Ram|     Sales|  3000|18800|\n",
      "|        Shyam|     Sales|  4100|18800|\n",
      "+-------------+----------+------+-----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|Employee_Name|Department|Salary| Min|\n",
      "+-------------+----------+------+----+\n",
      "|        Kunal|   Finance|  3000|3000|\n",
      "|      Sandeep|   Finance|  3900|3000|\n",
      "|      Srishti|Management|  3300|3300|\n",
      "|       Hitesh| Marketing|  3000|2000|\n",
      "|      Kailash| Marketing|  2000|2000|\n",
      "|          Ram|     Sales|  3000|3000|\n",
      "|        Meena|     Sales|  4600|3000|\n",
      "|     Abhishek|     Sales|  4100|3000|\n",
      "|          Ram|     Sales|  3000|3000|\n",
      "|        Shyam|     Sales|  4100|3000|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|Employee_Name|Department|Salary| Max|\n",
      "+-------------+----------+------+----+\n",
      "|        Kunal|   Finance|  3000|3900|\n",
      "|      Sandeep|   Finance|  3900|3900|\n",
      "|      Srishti|Management|  3300|3300|\n",
      "|       Hitesh| Marketing|  3000|3000|\n",
      "|      Kailash| Marketing|  2000|3000|\n",
      "|          Ram|     Sales|  3000|4600|\n",
      "|        Meena|     Sales|  4600|4600|\n",
      "|     Abhishek|     Sales|  4100|4600|\n",
      "|          Ram|     Sales|  3000|4600|\n",
      "|        Shyam|     Sales|  4100|4600|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.withColumn(\"Sum\",sum(col(\"salary\")).over(windowPartitionAgg)).show()\n",
    "df3.withColumn(\"Min\",min(col(\"salary\")).over(windowPartitionAgg)).show()\n",
    "df3.withColumn(\"Max\",max(col(\"salary\")).over(windowPartitionAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, DateType, BooleanType\n",
    "df = spark.createDataFrame([[\"1\", \"2019-07-01 12:01:19.000\"],\n",
    " [\"2\", \"2019-06-24 12:01:19.000\"]], [\"id\", \"input_timestamp\"])\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting string datatype to timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn(\"timestamptype\",to_timestamp(\"input_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      " |-- timestamptype: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# selecting only necessary column and renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.select(\"id\",\"timestamptype\").withColumnRenamed(\"timestamptype\",\"input_timestamp\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using cast to convert timestamp to DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df2.select(col(\"id\"), col(\"input_timestamp\").cast('string'))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# timestamp type to datetype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- to_date(input_timestamp): date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df2.select(col(\"id\"), to_date(col(\"input_timestamp\")))\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
